import openai
import numpy as np
import os
import requests
from typing import List, Dict
import json


class YandexRAG:
    """
    RAG —Å–∏—Å—Ç–µ–º–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Yandex Foundation Models
    —á–µ—Ä–µ–∑ OpenAI Compatible API
    """

    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG —Å–∏—Å—Ç–µ–º—ã"""
        self.api_key = "your_yandex_api_key"
        self.folder_id = "your_yandex_folder_id"
        self.base_url = "https://llm.api.cloud.yandex.net/foundationModels/v1/"

        if not self.api_key:
            raise ValueError("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è YANDEX_CLOUD_API_KEY")

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ OpenAI –∫–ª–∏–µ–Ω—Ç–∞ –¥–ª—è Yandex
        self.client = openai.OpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )

        self.embedding_model = f"emb://{self.folder_id}/text-search-doc/latest"
        self.generation_model = f"gpt://{self.folder_id}/yandexgpt/latest"
        self.documents = []

        print(f"‚úÖ YandexRAG –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –º–æ–¥–µ–ª—å—é: yandexgpt")

    def get_embeddings(self, texts: List[str]) -> List[List[float]]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —á–µ—Ä–µ–∑ Yandex API

        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏

        Returns:
            –°–ø–∏—Å–æ–∫ –≤–µ–∫—Ç–æ—Ä–æ–≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        """
        embeddings = []
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

        for text in texts:
            payload = {
                "modelUri": self.embedding_model,
                "text": text
            }

            try:
                response = requests.post(
                    f"{self.base_url}textEmbedding",
                    headers=headers,
                    json=payload,
                    timeout=30
                )
                response.raise_for_status()

                embedding = response.json()["embedding"]
                embeddings.append(embedding)

            except requests.exceptions.RequestException as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–π –≤–µ–∫—Ç–æ—Ä –∫–∞–∫ fallback
                embeddings.append(np.random.rand(256).tolist())

        return embeddings

    def split_documents(self, docs: List[str], chunk_size: int = 1000) -> List[str]:
        """
        –†–∞–∑–±–∏–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —á–∞–Ω–∫–∏

        Args:
            docs: –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            chunk_size: –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Å–ª–æ–≤–∞—Ö

        Returns:
            –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤
        """
        chunks = []
        for doc in docs:
            words = doc.split()
            for i in range(0, len(words), chunk_size):
                chunk = " ".join(words[i:i + chunk_size])
                if chunk.strip():  # –ò–∑–±–µ–≥–∞–µ–º –ø—É—Å—Ç—ã—Ö —á–∞–Ω–∫–æ–≤
                    chunks.append(chunk)

        print(f"üìÑ –°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤ –∏–∑ {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        return chunks

    def add_documents(self, docs: List[str]) -> None:
        """
        –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É

        Args:
            docs: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        print(f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")

        # –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏
        chunks = self.split_documents(docs)

        # –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        embeddings = self.get_embeddings(chunks)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î
        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
            self.documents.append({
                "id": i,
                "text": chunk,
                "embedding": np.array(embedding)
            })

        print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É")

    def search(self, query: str, top_k: int = 3) -> List[Dict]:
        """
        –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        if not self.documents:
            print("‚ö†Ô∏è –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –ø—É—Å—Ç–∞—è!")
            return []

        print(f"üîç –ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}'")

        # –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∑–∞–ø—Ä–æ—Å–∞
        query_embeddings = self.get_embeddings([query])
        query_vector = np.array(query_embeddings[0])

        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ö–æ–¥—Å—Ç–≤–∞
        similarities = []
        for doc in self.documents:
            similarity = np.dot(query_vector, doc["embedding"]) / (
                    np.linalg.norm(query_vector) * np.linalg.norm(doc["embedding"])
            )
            similarities.append({
                "document": doc,
                "similarity": float(similarity)
            })

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–¥—Å—Ç–≤–∞
        similarities.sort(key=lambda x: x["similarity"], reverse=True)

        results = similarities[:top_k]
        print(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

        return results

    def generate_answer(self, query: str, context_docs: List[Dict]) -> str:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

        Args:
            query: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            context_docs: –ù–∞–π–¥–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã

        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
        """
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        context_texts = []
        for i, doc_info in enumerate(context_docs, 1):
            doc = doc_info["document"]
            similarity = doc_info["similarity"]
            context_texts.append(f"–î–æ–∫—É–º–µ–Ω—Ç {i} (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {similarity:.3f}): {doc['text']}")

            context = """

                      """.join(context_texts)

            # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞
            messages = [
                {
                    "role": "system",
                    "content": "–¢—ã - –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –ø–æ–∏—Å–∫—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –û—Ç–≤–µ—á–∞–π –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç. –ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –æ—Ç–≤–µ—Ç–∞, —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º."
                },
                {
                    "role": "user",
                    "content": f"""–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}

–í–æ–ø—Ä–æ—Å: {query}

–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å —Ç–æ–ª—å–∫–æ –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ:"""
                }
            ]

        try:
            print("ü§ñ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞...")
            response = self.client.chat.completions.create(
                model=self.generation_model,
                messages=messages,
                max_tokens=1500,
                temperature=0.2
            )

            answer = response.choices[0].message.content
            return answer

        except Exception as e:
            error_msg = f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}"
            print(error_msg)
            return error_msg

    def ask(self, query: str) -> Dict:
        """
        –ü–æ–ª–Ω—ã–π RAG –∑–∞–ø—Ä–æ—Å: –ø–æ–∏—Å–∫ + –≥–µ–Ω–µ—Ä–∞—Ü–∏—è

        Args:
            query: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç —Å –æ—Ç–≤–µ—Ç–æ–º –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        search_results = self.search(query)

        if not search_results:
            return {
                "answer": "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã.",
                "sources": [],
                "query": query
            }

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        answer = self.generate_answer(query, search_results)

        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        sources = []
        for result in search_results:
            doc = result["document"]
            sources.append({
                "text": doc["text"][:200] + "..." if len(doc["text"]) > 200 else doc["text"],
                "similarity": result["similarity"],
                "id": doc["id"]
            })

        return {
            "answer": answer,
            "sources": sources,
            "query": query,
            "model": "yandexgpt"
        }


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
def demo_yandex_rag():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã YandexRAG"""

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    rag = YandexRAG()

    # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
    documents = [
        "Yandex Foundation Models - —ç—Ç–æ —Å–µ–º–µ–π—Å—Ç–≤–æ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –æ—Ç –Ø–Ω–¥–µ–∫—Å–∞ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞.",
        "YandexGPT Pro –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–æ 32000 —Ç–æ–∫–µ–Ω–æ–≤ –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º –∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–∞—Ö.",
        "RAG (Retrieval-Augmented Generation) –ø–æ–∑–≤–æ–ª—è–µ—Ç —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–Ω–µ—à–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∑–Ω–∞–Ω–∏–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤.",
        "–í–µ–∫—Ç–æ—Ä–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É—é—Ç —Ç–µ–∫—Å—Ç –≤ —á–∏—Å–ª–æ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º.",
        "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –Ω–∞—Ö–æ–¥–∏—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º, –∞ –ø–æ —Å–º—ã—Å–ª—É –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –∑–∞–ø—Ä–æ—Å–∞."
    ]

    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    rag.add_documents(documents)

    # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    queries = [
        "–ß—Ç–æ —Ç–∞–∫–æ–µ YandexGPT Pro?",
        "–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç RAG?",
        "–ß—Ç–æ —Ç–∞–∫–æ–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫?"
    ]

    print("""
          " + " = """*80)
    print("üéØ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø YANDEX RAG –°–ò–°–¢–ï–ú–´")
    print("=" * 80)

    for query in queries:
        print(f"‚ùì –ó–∞–ø—Ä–æ—Å: {query}")
    print("-" * 60)

    result = rag.ask(query)

    print(f"ü§ñ –û—Ç–≤–µ—Ç: {result['answer']}")
    print(f"üìö –ò—Å—Ç–æ—á–Ω–∏–∫–∏({len(result['sources'])}): ")

    for i, source in enumerate(result['sources'], 1):
        print(f"   {i}. –°—Ö–æ–¥—Å—Ç–≤–æ: {source['similarity']:.3f}")
    print(f"      {source['text']}")

    print(" " + " - " * 60)


demo_yandex_rag()
